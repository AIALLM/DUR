{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os, openai, getpass, tiktoken\n",
    "import pandas as pd\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "#==========================================================================================#\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path='../data/dur.csv')\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 250, chunk_overlap = 100, length_function=tiktoken_len)\n",
    "texts = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"jhgan/ko-sbert-nli\"\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to disk\n",
    "#emed_db = Chroma.from_documents(texts, hf,persist_directory=\"../db/dur_jhgan_250\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#벡터db의 데이터가 in-memory가 아니라 persistent storage인 disk에 저장되게 선언\n",
    "#emed_db.persist()\n",
    "#emed_db = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk\n",
    "emed_db = Chroma(persist_directory=\"../db/dur_jhgan_250\",embedding_function=hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = ChatOpenAI(model = \"gpt-3.5-turbo\",temperature=0)\n",
    "\n",
    "vector_retriever = emed_db.as_retriever(search_type=\"similarity\", search_kwargs={'k':3})\n",
    "\n",
    "# Initialize the BM25 retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(data)\n",
    "bm25_retriever.k =  3\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, vector_retriever], weights=[0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble_retriever.invoke(\"타세놀\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"타세놀 노인이 먹어도 돼?\"\n",
    "# docs = ensemble_retriever.get_relevant_documents(query)\n",
    "# docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\",\"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    openai, ensemble_retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "#답변 생성\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "ONLY USE the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(openai, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever,question_answer_chain)\n",
    "\n",
    "# chat history 관리\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id : str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key= \"input\",\n",
    "    history_messages_key= \"chat_history\",\n",
    "    output_messages_key= \"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id = \"0\"\n",
    "# query1 = \"타세놀 노인이 복용해도 돼??\"\n",
    "# try: \n",
    "#     result = conversational_rag_chain.invoke(\n",
    "#         {\"input\": query1},\n",
    "#         config={\n",
    "#             \"configurable\": {\"session_id\":id}\n",
    "#         },\n",
    "#     )#[\"answer\"]\n",
    "#     print(result)\n",
    "# except Exception as e:\n",
    "#     print(\"오류 발생\\n사유 : \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def measure_time(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"{func.__name__} 실행 시간: {elapsed_time} 초\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@measure_time\n",
    "def invoke(id, input):\n",
    "    response = conversational_rag_chain.invoke(\n",
    "        {\"input\": input},\n",
    "        config={\n",
    "            \"configurable\": {\"session_id\":id}\n",
    "        },\n",
    "    )#[\"answer\"]\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_excel(\"../data/qa_dataset_drop_company.xlsx\")\n",
    "\n",
    "answer = [] # llm의 응답을 저장할 리스트\n",
    "docs = [] # retrieve한 문서 내용 저장한 리스트\n",
    "for i, row in test_data.iterrows():\n",
    "    try:\n",
    "        response = invoke(str(i), row.question)\n",
    "        answer.append(response[\"answer\"])\n",
    "        docs.append(response)\n",
    "        print(i+1,'번째 답변 : ',answer[i])\n",
    "    except Exception as e:\n",
    "        print(i+1,'번째 답변 오류. 오류 사유 : ', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = test_data['question']\n",
    "test_output = pd.DataFrame({\"question\": question,\n",
    "                            \"answer\" : answer,\n",
    "                            \"docs\" : docs})\n",
    "test_output.to_excel(\"../result/dur_1차_jhgan.xlsx\",index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
